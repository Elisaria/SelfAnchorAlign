from keras.src.layers.layer import Layer
from keras.src.layers import LayerNormalization, Dense
from tensorflow import gather, linalg, shape, Variable, reduce_mean
from keras.src import ops


# input(B, L, in_dim)
# output(B, L, dim)
class Attention_Block(Layer):

    def __init__(self, num_heads=4, dim=512, trainable=True, causal_mask=False, **kwargs):
        super().__init__(**kwargs)
        # ============ Precompute =================#
        # rotation indices [1, 0, 3, 2, 5, 4, ...]
        self.indices = ops.convert_to_tensor(
            [i ^ 1 for i in range(dim)]
        )
        self.cos, self.sin = self.rope_scales([72, dim])
        # =========================================#
        self.dim = dim
        self.num_heads = num_heads
        self.trainable = trainable
        self.causal_mask = causal_mask

    def build(self, input_shape):
        in_dim = input_shape[-1]
        self.query_kernel = self.add_weight(shape=[in_dim, self.dim],
                                            trainable=self.trainable,
                                            regularizer='l2',
                                            name='query_kernel')
        self.key_kernel = self.add_weight(shape=[in_dim, self.dim],
                                          trainable=self.trainable,
                                          regularizer='l2',
                                          name='key_kernel')
        self.up_kernel = self.add_weight(shape=[self.dim, 2 * self.dim],
                                         trainable=self.trainable,
                                         name='up_kernel')
        self.up_bias = self.add_weight(shape=[2 * self.dim, ],
                                       trainable=self.trainable,
                                       name='up_bias')
        self.down_kernel = self.add_weight(shape=[2 * self.dim, self.dim],
                                           trainable=self.trainable,
                                           name='down_kernel')
        self.down_bias = self.add_weight(shape=[self.dim, ],
                                         trainable=self.trainable,
                                         name='down_bias')
        self.gamma = self.add_weight(shape=[self.dim, ],
                                     trainable=self.trainable,
                                     name='gamma')
        self.beta = self.add_weight(shape=[self.dim, ],
                                    trainable=self.trainable,
                                    name='beta')

    def load_own_variables(self, store):
        """ store: dict, {
                          'query_kernel': arr(in_dim, dim),
                          'key_kernel': arr(in_dim, dim),
                          'up_kernel': arr(dim, 2*dim),
                          'up_bias' : arr(2*dim, )
                          'down_kernel': arr(2*dim, dim),
                          'down_bias' : arr(dim, ),
                          'gamma' : arr(dim, )
                          'beta' : arr(dim, )
                         }
                """
        for weight in self.weights:
            weight.assign(store[weight.path.split('/')[-1]])

    def rope_scales(self, input_shape):
        seq_len = input_shape[-2]
        dim = input_shape[-1]
        inv_freq = 10000. ** (-ops.arange(0, dim, 2, dtype='float32') / dim)  # (half_dim,)
        positions = ops.arange(seq_len, dtype='float32')  # [0, 1, ..., seq_len-1] (seq_len,)
        freqs = ops.einsum('i,j->ij', positions, inv_freq)  # 形状: (seq_len, half_dim)
        cos = ops.repeat(ops.cos(freqs), 2, axis=-1)
        sin_freqs = ops.sin(freqs)
        sin = ops.stack([-sin_freqs, sin_freqs], axis=-1)  # (seq_len, half_dim, 2)
        sin = ops.reshape(sin, (seq_len, dim))
        return cos, sin

    def rotate(self, inputs):
        return gather(inputs, self.indices, axis=-1)

    # mask(B, L)
    def padding_mask(self, mask):
        # (B, L, L)
        mask = ops.expand_dims(mask, axis=-1) & ops.expand_dims(mask, axis=-2)
        # (B, num_heads, L, L)
        mask = ops.tile(ops.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])
        return mask

    def LayerNorm(self, inputs):
        x = (inputs - ops.mean(inputs, axis=-1, keepdims=True)) / ops.sqrt(
            ops.var(inputs, axis=-1, keepdims=True) + 1e-3)
        x = self.gamma * x + self.beta
        return x

    def call(self, inputs, training=False):
        mask = getattr(inputs, "_keras_mask", None)
        B = shape(inputs)[0]
        L = shape(inputs)[1]

        value = self.LayerNorm(inputs)
        value = ops.reshape(value, (B, L, self.num_heads, self.dim // self.num_heads))

        query = ops.matmul(inputs, self.query_kernel)
        query = query * self.cos[:L] + self.rotate(query) * self.sin[:L]
        query = ops.reshape(query, (B, L, self.num_heads, self.dim // self.num_heads))

        key = ops.matmul(inputs, self.key_kernel)
        key = key * self.cos[:L] + self.rotate(key) * self.sin[:L]
        key = ops.reshape(key, (B, L, self.num_heads, self.dim // self.num_heads))

        k = ops.sqrt(ops.cast(self.dim, dtype='float32'))
        dots = ops.einsum("aecd, abcd -> acbe", key, query) / k
        dots_mask = ops.full(shape(dots), True, dtype='bool')
        if mask is not None:
            dots_mask = dots_mask & self.padding_mask(mask)
        if self.causal_mask:
            dots_mask = dots_mask & linalg.band_part(ops.ones((L, L), 'bool'), -1, 0)

        scores = ops.softmax(dots + (1.0 - ops.cast(dots_mask, 'float32')) * -1e10, axis=-1)

        x = ops.einsum("acbe,aecd->abcd", scores, value)
        x = ops.reshape(x, (B, L, self.dim))

        x = self.LayerNorm(x + inputs)
        temp = ops.matmul(x, self.up_kernel) + self.up_bias
        temp = ops.matmul(temp, self.down_kernel) + self.down_bias
        y = ops.gelu(x + temp)
        setattr(y, '_keras_mask', mask)
        return y


class Text_Encoder(Layer):
    def __init__(self, num_layers=4):
        super().__init__()
        self.ABs = [Attention_Block(name='AB' + str(_)) for _ in range(num_layers)]
        self.LayerNorm = LayerNormalization(name='LN')
        self.linear_projection = Dense(512, name='linear_projection')

    # input : (B, L, 512)
    # output : (B, L, 512)
    def call(self, x, *args, **kwargs):
        for AB in self.ABs:
            x = AB(x)
        x = self.LayerNorm(x)
        return self.linear_projection(x)


class Vit(Layer):
    def __init__(self, num_layers=4):
        super().__init__()
        self.in_ = Dense(512, name='in_')
        self.ABs = [Attention_Block(name='AB' + str(_)) for _ in range(num_layers)]
        self.LayerNorm = LayerNormalization(name='LN')
        self.linear_projection = Dense(512, name='linear_projection')

    def build(self, input_shape):
        self.CLS = self.add_weight(name='CLS',
                                   shape=[1, 1, 512])

    # input : patches (B, L, patch_pixels)
    # output : (B, L+1, 512)
    def call(self, x, *args, **kwargs):
        # (B, L, 512)
        x = self.in_(x)
        x = ops.concatenate([ops.tile(self.CLS, [shape(x)[0], 1, 1]), x], axis=-2)
        for AB in self.ABs:
            x = AB(x)
        x = self.LayerNorm(x)
        return self.linear_projection(x)


class Text_Decoder(Layer):
    def __init__(self, vocab_size, num_layers=4):
        super().__init__()
        self.ABs = [Attention_Block(causal_mask=True, name='AB' + str(_)) for _ in range(num_layers)]
        self.LayerNorm = LayerNormalization(name='LN')
        self.predict_head = Dense(vocab_size, activation='softmax', name='predict_head')

    # input : (B, L, 512)
    # output : (B, L, vocab_size)
    def call(self, x, *args, **kwargs):
        for AB in self.ABs:
            x = AB(x)
        x = self.LayerNorm(x)
        return self.predict_head(x)


if __name__ == '__main__':
    data = ops.random.uniform((1, 4, 512))
    store = {
        'query_kernel': Variable(ops.ones((512, 512), 'float32')),
        'key_kernel': 2. * Variable(ops.ones((512, 512), 'float32')),
        'up_kernel': 3. * Variable(ops.ones((512, 1024), 'float32')),
        'up_bias': 4. * Variable(ops.ones((1024,), 'float32')),
        'down_kernel': 5. * Variable(ops.ones((1024, 512), 'float32')),
        'down_bias': 6. * Variable(ops.ones((512,), 'float32')),
        'gamma': 7. * Variable(ops.ones((512,), 'float32')),
        'beta': 8. * Variable(ops.ones((512,), 'float32'))
    }

    model = Attention_Block()
    model.build((1, 4, 512))
    model.load_own_variables(store)

    print(reduce_mean(model(data)))
    model(data)
