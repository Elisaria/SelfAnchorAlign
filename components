from keras.src.layers.layer import Layer
from keras.src.layers import Dense
from tensorflow import gather, linalg, shape, Variable, reduce_mean
from keras.src import ops


# input(B, L, in_dim)
# output(B, L, dim)
class Attention_Block(Layer):

    def __init__(self, num_heads=4, dim=512, trainable=True, causal_mask=False, **kwargs):
        super().__init__(**kwargs)
        self.dim = dim
        self.half_dim = dim // 2
        self.num_heads = num_heads
        self.trainable = trainable
        self.causal_mask = causal_mask

        # ============ Precompute =================#
        # rotation indices [1, 0, 3, 2, 5, 4, ...]
        self.indices = ops.convert_to_tensor(
            [i ^ 1 for i in range(self.half_dim)]
        )
        self.cos, self.sin = self.rope_scales([50, self.half_dim])
        # =========================================#

    def build(self, input_shape):
        in_dim = input_shape[-1]
        self.query_kernel = self.add_weight(shape=[in_dim, self.half_dim],
                                            trainable=self.trainable,
                                            name='query_kernel')
        self.key_kernel = self.add_weight(shape=[in_dim, self.half_dim],
                                          trainable=self.trainable,
                                          name='key_kernel')
        self.value_kernel = self.add_weight(shape=[in_dim, self.dim],
                                            trainable=self.trainable,
                                            name='value_kernel')
        self.W1 = self.add_weight(shape=[self.dim, 2 * self.dim],
                                  trainable=self.trainable,
                                  name='W1')
        self.W2 = self.add_weight(shape=[self.dim, 2 * self.dim],
                                  trainable=self.trainable,
                                  name='W2')
        self.W3 = self.add_weight(shape=[2 * self.dim, self.dim],
                                  trainable=self.trainable,
                                  name='W3')
        self.gamma = self.add_weight(shape=[self.dim, ],
                                     trainable=self.trainable,
                                     name='gamma',
                                     initializer='ones')

    def load_own_variables(self, store):
        """ store: dict, {
                          'query_kernel': arr(in_dim, half_dim),
                          'key_kernel': arr(in_dim, half_dim),
                          'value_kernel': arr(dim, dim),
                          'W1': arr(dim, 2*dim),
                          'W2': arr(dim, 2*dim),
                          'W3': arr(dim, dim),
                          'gamma' : arr(dim, )
                         }
                """
        for weight in self.weights:
            weight.assign(store[weight.path.split('/')[-1]])

    def rope_scales(self, input_shape):
        seq_len = input_shape[-2]
        dim = input_shape[-1]
        inv_freq = 10000. ** (-ops.arange(0, dim, 2, dtype='float32') / dim)  # (half_dim,)
        positions = ops.arange(seq_len, dtype='float32')  # [0, 1, ..., seq_len-1] (seq_len,)
        freqs = ops.einsum('i,j->ij', positions, inv_freq)  # 形状: (seq_len, half_dim)
        cos = ops.repeat(ops.cos(freqs), 2, axis=-1)
        sin_freqs = ops.sin(freqs)
        sin = ops.stack([-sin_freqs, sin_freqs], axis=-1)  # (seq_len, half_dim, 2)
        sin = ops.reshape(sin, (seq_len, dim))
        return cos, sin

    def rotate(self, inputs):
        return gather(inputs, self.indices, axis=-1)

    # mask(B, L)
    def padding_mask(self, mask):
        # (B, L, L)
        mask = ops.expand_dims(mask, axis=-1) & ops.expand_dims(mask, axis=-2)
        # (B, num_heads, L, L)
        mask = ops.tile(ops.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])
        return mask

    def RMSNorm(self, inputs):
        x = inputs / ops.sqrt(
            ops.var(inputs, axis=-1, keepdims=True) + 1e-3)
        x = self.gamma * x
        return x

    def call(self, inputs, training=False):
        mask = getattr(inputs, "_keras_mask", None)
        B = shape(inputs)[0]
        L = shape(inputs)[1]

        value = self.RMSNorm(inputs)
        value = ops.matmul(value, self.value_kernel)
        value = ops.reshape(value, (B, L, self.num_heads, self.dim // self.num_heads))

        query = ops.matmul(inputs, self.query_kernel)
        query = query * self.cos[:L] + self.rotate(query) * self.sin[:L]
        query = ops.reshape(query, (B, L, self.num_heads, self.half_dim // self.num_heads))

        key = ops.matmul(inputs, self.key_kernel)
        key = key * self.cos[:L] + self.rotate(key) * self.sin[:L]
        key = ops.reshape(key, (B, L, self.num_heads, self.half_dim // self.num_heads))

        k = ops.sqrt(ops.cast(self.half_dim, dtype='float32'))
        dots = ops.einsum("aecd, abcd -> acbe", key, query) / k
        dots_mask = ops.full(shape(dots), True, dtype='bool')
        if mask is not None:
            dots_mask = dots_mask & self.padding_mask(mask)
        if self.causal_mask:
            dots_mask = dots_mask & linalg.band_part(ops.ones((L, L), 'bool'), -1, 0)

        scores = ops.softmax(dots + (1.0 - ops.cast(dots_mask, 'float32')) * -1e10, axis=-1)

        x = ops.einsum("acbe,aecd->abcd", scores, value)
        x = ops.reshape(x, (B, L, self.dim))

        x = self.RMSNorm(x + inputs)
        temp = ops.matmul(x, self.W2) * ops.sigmoid(ops.matmul(x, self.W1))
        temp = ops.matmul(temp, self.W3)
        y = x + temp
        setattr(y, '_keras_mask', mask)
        return y


class Text_Encoder(Layer):
    def __init__(self, num_layers=4):
        super().__init__()
        self.ABs = [Attention_Block(name='AB' + str(_)) for _ in range(num_layers)]
        self.linear_projection = Dense(512, name='linear_projection')

    # input : (B, L, 512)
    # output : (B, L, 512)
    def call(self, x, *args, **kwargs):
        for AB in self.ABs:
            x = AB(x)
        return self.linear_projection(x)


class Vit(Layer):
    def __init__(self, num_layers=8):
        super().__init__()
        self.in_ = Dense(512, name='in_')
        self.ABs = [Attention_Block(name='AB' + str(_)) for _ in range(num_layers)]
        self.linear_projection = Dense(512, name='linear_projection')

    def build(self, input_shape):
        self.CLS = self.add_weight(name='CLS',
                                   shape=[1, 1, 512])

    # input : patches (B, L, patch_pixels)
    # output : (B, L+1, 512)
    def call(self, x, *args, **kwargs):
        # (B, L, 512)
        x = self.in_(x)
        x = ops.concatenate([ops.tile(self.CLS, [shape(x)[0], 1, 1]), x], axis=-2)
        for AB in self.ABs:
            x = AB(x)
        return self.linear_projection(x)


class Text_Decoder(Layer):
    def __init__(self, vocab_size, num_layers=4):
        super().__init__()
        self.ABs = [Attention_Block(causal_mask=True, name='AB' + str(_)) for _ in range(num_layers)]
        self.predict_head = Dense(vocab_size, activation='softmax', name='predict_head')

    # input : (B, L, 512)
    # output : (B, L, vocab_size)
    def call(self, x, *args, **kwargs):
        for AB in self.ABs:
            x = AB(x)
        return self.predict_head(x)


if __name__ == '__main__':
    data = ops.random.uniform((1, 4, 512))
    store = {
        'query_kernel': Variable(ops.ones((512, 256), 'float32')),
        'key_kernel': 2. * Variable(ops.ones((512, 256), 'float32')),
        'value_kernel': 3. * Variable(ops.ones((512, 512), 'float32')),
        'W1': 4. * Variable(ops.ones((512, 1024), 'float32')),
        'W2': 5. * Variable(ops.ones((512, 1024), 'float32')),
        'W3': 6. * Variable(ops.ones((1024, 512), 'float32')),
        'gamma': 7. * Variable(ops.ones((512,), 'float32'))
    }

    model = Attention_Block()
    model.build((1, 4, 512))
    model.load_own_variables(store)

    print(reduce_mean(model(data)))
    model(data)
